---
title: "Analiza win"
output:
  html_document:
    df_print: paged
    toc: true #włącza spis treści
    #toc_float: true #umożliwia przewijany spis treści
    number_sections: true #numeruje sekcje
  pdf_document: default
---


Zbiór danych wykorzystany do poniższej analizy pochodzi ze strony [kaggle.com](https://www.kaggle.com/datasets/shelvigarg/wine-quality-dataset/data). Jest to zbiór zawierający dane o czerwonym oraz białym wariancie wina "Vinho Verde", pochodzącego z północy Portugalii.

Celem niniejszej analizy jest zastosowanie regresji liniowej.

Załadujemy na początek potrzebne biblioteki, żeby funkcje, które są wykorzystywane w projekcie, działały poprawnie.

```{r,message = FALSE, warning = FALSE}
library(car)
library(corrplot)
library(dplyr)
library(faraway)
library(lmtest)
library(MASS)
library(nortest)
library(RColorBrewer)
```

# Dane - ich struktura oraz klasyfikacja

Załadujmy plik winequalityN.csv oraz prześledźmy jego strukturę danych.

```{r}
wine <- read.csv("C:\\Users\\Nikola\\Documents\\Nikola Chmielewska\\R\\Datasets\\winequalityN.csv")
str(wine)
unique(wine$type)
```
Dokonajmy jeszcze objaśnienia zmiennych występujących w zbiorze danych.

| Nazwa | Opis | 
|--------|--------|
| type | rodzaj wina (białe, czerwone) |
| fixed.acidity | stała kwasowość |
| volatile acidity | zmienna kwasowość |
| citric acid | kwas cytrynowy |
| residual sugar | resztowy cukier |
| chlorides |	chlorki |
| free sulfur dioxide | 	wolny dwutlenek siarki |
| total sulfur dioxide | 	całkowity dwutlenek siarki |
| density |	gęstość |
| pH – potential of hydrogen | współczynnik kwasowości/zasadowości pH |
| sulphates | siarczyny |
| alcohol | alkohol |
| quality |	jakość |

Oraz, aby lepiej zrozumieć dane, zobaczmy ich podsumowanie:

```{r}
summary(wine)
```

Sprawdźmy jeszcze, czy w zbiorze wszystkie dane są podane:

```{r}
(sapply(wine, function(x) {sum(is.na(x))}))
```
Jak widać powyżej, nasza tabela zawiera wartości NA. Usuniemy zatem wiersze, które nie posiadają danych. Robimy to, ponieważ brak danych może obniżyć jakość późniejszej analizy danych.

```{r}
wine_clean <- na.omit(wine) #usuwa wiersze zawierające wartości NA
```

Wyświetlmy jeszcze raz nasz nowy zbiór danych z usuniętymi wartościami NA.

```{r}
(sapply(wine_clean, function(x) {sum(is.na(x))}))
```

Jak widzimy, w tej chwili żadna zmienna nie jest obarczona brakiem danych, więc nasz zbiór danych jest gotowy do obróbki.

Jednocześnie w niniejszym zestawie danych zmienną zależną jest atrybut quality, a więc ocena jakości wina. Pozostałe zmienne są zmiennymi niezależnymi wpływającymi na końcową ocenę wina; są to również zmienne ilościowe. Natomiast zmienną objaśnianą ‘quality’ można potraktować zarówno jako zmienną ilościową oraz jako zmienną nominalną o uporządkowanych kategoriach (jakościową). 

W niniejszej pracy atrybut quality potraktujemy jako zmienną ilościową, rozważając modele regresji liniowej. Jednak podczas tworzenia modelu proporcjonalnych szans, zmienną ‘quality’ potraktujemy jako zmienną jakościową. Warto w tym miejscu zaznaczyć, że w celu weryfikacji hipotez będziemy zakładać poziom istotności α=0,05.

## Podział zbioru danych

Podzielimy nasz zbiór danych na podstawie wariantu wina - białe i czerwone. Zrobimy osobną analizę dla każdego wariantu.

```{r}
white_wine <- subset(wine_clean, type == "white")
red_wine <- subset(wine_clean, type == "red")
```

Oprócz tego usuniemy kolumny tekstowe, ponieważ tylko takie kolumny będą nam potrzebne do dlszej analizy.

```{r}
white_numeric <- white_wine[sapply(white_wine, is.numeric)]
```


Stworzymy teraz zmienne pomocnicze, które pomogą nam podzielić zbiór danych na 3 podzbiory.

```{r}
N <- nrow(white_numeric)
I <- (1:N)
```

Teraz losujemy indeksy.

```{r}
set.seed(300)
I_l <- sample.int(N, size = round(N/2)) #50% danych
I_v <- sample(setdiff(I, I_l), size = round(N/4)) #25% danych
I_t <- setdiff(setdiff(I, I_l), I_v) #25% danych
```

Przypiszmy im konkretne dane, dzieląc nasz wyjściowy zbiór danych na próbę uczącą, walidacyjną oraz testową o udziale procentowym, odpowiednio, 50%, 25% oraz 25% danych wyjściowych.

```{r}
lrn <- white_numeric[I_l,] #próba ucząca
val <- white_numeric[I_v,] #próba walidacyjna
tst <- white_numeric[I_t,] #próba testowa
```

W ten sposób podzieliliśmy zbiór danych na trzy podzbiory.

## Korelacja zmiennych

Zanim utworzymy model regresji liniowej, zobaczmy jak prezentuje się wykres korelacji poszczególnych zmiennych w naszej próbie uczącej.

```{r}
white_matrix <- cor(lrn)
corrplot(white_matrix, type = "lower")
```

Wartość dodatnia oznacza, że wraz ze wzrostem jednej cechy, następuje wzrost drugiej. Wartość ujemna natomiast określa, że wraz ze wzrostem jednej, następuje spadek drugiej cechy. W takim razie wartości najbliżej krańców przedziału wskazują największą korelacje. Z drugiej strony wartości bliżej zera wskazują na brak powiązania zmiennych.

Widzimy, że najbardziej skorelowaną zmienną, ze zmienną ‘quality’, jest ‘alcohol’ oraz ‘density’, gdzie są to zmienne odpowiednio skorelowane dodatnio oraz ujemnie.

Warto też zauważyć, że mocną wzajemną korelację wykazują zmienne ‘density’ i ‘residual.sugar’ oraz zmienne ‘alcohol’ i ‘density’. W takim razie stworzymy modele bez ‘residual.sugar’ i ‘alcohol’, żeby zobaczyć, jakie wyniki wtedy dostaniemy.

Zastosujemy również metodę składowych głównych (PCA), żeby porównać oba podejścia.


# Modele regresji liniowej
## Modele podstawowe

Stworzymy modele regresji liniowej zmiennej ‘quality’ względem, odpowiednio:

- wszystkich zmiennych występujących w próbce uczącej (full),
- wszystkich zmiennych występujących w próbce uczącej z wyrzuceniem zmiennej ‘residual.sugar’ (no sugar),
- wszystkich zmiennych występujących w próbce uczącej z wyrzuceniem zmiennej ‘alcohol’ (no alcohol),
- wszystkich zmiennych występujących w próbce uczącej z wyrzuceniem zmiennych ‘residual.sugar’ oraz ‘alcohol’ (no sugar, no alcohol).

Tworzymy w tym celu funkcję, żeby zautomatyzować ten proces.

```{r}
models <<- list()
countM  = 0
add_model = function(name, model, ispca){
  models <<- append(models, list(name, model, ispca))
  countM <<- countM + 1
}
get_name  = function(index){models[[index*3-3+1]]}
get_model = function(index){models[[index*3-3+2]]}
is_pca    = function(index){models[[index*3-3+3]]}

add_model('full',                 lm(quality ~ .,                            data = lrn), FALSE)
add_model('no sugar',             lm(quality ~ . - residual.sugar,           data = lrn), FALSE)
add_model('no alcohol',           lm(quality ~ .                  - alcohol, data = lrn), FALSE)
add_model('no sugar, no alcohol', lm(quality ~ . - residual.sugar - alcohol, data = lrn), FALSE)
```

Poniżej wyświetlamy podsumowanie każdego modelu w podanej wyżej kolejności.

```{r}
for(i in 1:countM){
  print(get_name(i))
  print(summary(get_model(i)))
}
```

Już na pierwszy rzut oka widać, że ostatni model nie jest w żaden sposób konkurencyjny względem reszty modeli, biorąc pod uwagę chociażby skorygowany współczynnik determinacji, który jest nieporównywalnie mniejszy w stosunku do reszty modeli. Jednak w celu zweryfikowania, czy modele mniejsze są adekwatne, posłużymy się testem ANOVA.

### ANOVA

Nasz test statystyczny ma postać:

- H<sub>0</sub>: mniejszy model jest adekwatny,
- H<sub>1</sub>: mniejszy model nie jest adekwatny.

```{r}
for(i in 2:countM){
  print(paste(get_name(1), 'vs', get_name(i), sep=' '))
  print(anova(get_model(1), get_model(i)))
}
```

Widzimy stąd, że na poziomie istotności α=0.05 jesteśmy w stanie odrzucić hipotezę H<sub>0</sub>. Oznacza to, że żaden z modeli powstałych przez wyrzucenie zmiennych mocno skorelowanych z density nie jest adekwatny.

### Metoda wstecznej eliminacji i ANOVA

W tym podrozdziale zastosujemy metodę wstecznej eliminacji. Polega ona na kolejnym wyrzucaniu zmiennych, które są najmniej istotne w modelu. Dzięki temu dostaniemy nowe modele. które posłużą nam do dalszej analizy.

Zaznaczmy, że będziemy wyrzucać zmienne z pierwszego modelu, czyli z modelu regresji liniowej zmiennej ‘quality’ względem wszystkich zmiennych występujących w próbce uczącej.

```{r}
elim <- add_model('no acid', lm(quality ~ . - citric.acid, data = lrn), FALSE)
anova(get_model(1), get_model(countM))
```

```{r}
summary(get_model(countM))
```

Przed wyświetleniem podsumowania zastosowaliśmy od razu test ANOVA, żeby zweryfikować, czy mniejszy model jest adekwatny. Na poziomie istotności α=0,05 nie jesteśmy w stanie odrzucić hipotezy H0.

```{r}
add_model('no acid, no chlorides', lm(quality ~ . - citric.acid - chlorides, data = lrn), FALSE)
anova(get_model(1), get_model(countM))
```

```{r}
summary(get_model(countM))
```

```{r}
add_model('no acid, no chlorides, no totalsulf', lm(quality ~ . - citric.acid - chlorides - total.sulfur.dioxide, data = lrn), FALSE)
anova(get_model(1), get_model(countM))
```

```{r}
summary(get_model(countM))
```

Podobna sytuacja występuje w dwóch kolejnych modelach. Nie jesteśmy w stanie już dalej odrzucać zmiennych, ponieważ na poziomie istotności α=0.05 kolejne zmienne są istotne, więc usuwanie ich nie jest sensowne.

## Analiza składowych głównych

W celu zbadania innego podejścia do tworzenia modelów wykorzystamy metodę składowych głównych.
